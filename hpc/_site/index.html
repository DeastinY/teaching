<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>Cluster Introduction</title>
    <link rel="shortcut icon" href="./favicon.ico" />
    <link rel="stylesheet" href="./dist/reset.css" />
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="./dist/theme/moon.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/base16/zenburn.css" />

  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section data-markdown data-separator="---" data-separator-vertical="<!--v-->">
          <textarea data-template>
            

## Getting started with High Performance Computing in Research
_Richard Polzin <rpolzin@ukaachen.de\>_  
###### Last Updated: 24. July 2024  

Note: test note

---

# Table of Content

1. The What, Why, and How of HPC <!-- .element: class="fragment" data-fragment-index="1" -->
2. Linux Crashcourse <!-- .element: class="fragment" data-fragment-index="2" -->
3. RWTH Compute Cluster <!-- .element: class="fragment" data-fragment-index="3" -->
4. Tips and Tricks <!-- .element: class="fragment" data-fragment-index="4" -->
5. SLURM Job Manager <!-- .element: class="fragment" data-fragment-index="5" -->
6. Compute Time Application <!-- .element: class="fragment" data-fragment-index="6" -->

---

## The What, ~Why, and How~ of HPC

- **What** is HPC?
    - **H**igh **P**erformance **C**omputing: Systems thousands of times faster than your laptop 
    - Large bandwidth, store hundreds of GB in RAM, Massive parallelization, ...
    - Usually running some form of Linux

<!--v-->

## The ~What,~ Why, ~and How~ of HPC

- **Why** would you want HPC?
    - Basically: If you could profit from your programs running faster or with more data
    - Highly Parallelizable, runs  "in the background"
    - 96 CPU cores per node, 1.5TB SSDs, up to 1TB RAM, 4 GPUs ...
<!--v-->

## The ~What, Why, and~ How of HPC

-  **How** do you get access to HPC?
    - All RWTH-affiliates are granted access to some extend
    - NHR-Verbund offers free compute resources for scientists working at universities in Germany

---

![alt text](image-1.png)

---

<!-- .slide: data-auto-animate -->

### RWTH Compute Cluster <!-- .element: data-id="code-animation"-->
#### Account Creation

Use Selfservice to create your account   
(https://idm.rwth-aachen.de/selfservice/)
1. Accounts and Passwords
2. Account Overview
3. Create Account

<!--v-->

<!-- .slide: data-auto-animate -->
### RWTH Compute Cluster <!-- .element: data-id="code-animation"-->
#### Login

```zsh
ssh ab1234@login23-1.hpc-itc.rwth-aachen.de
```

- Login with secure shell (ssh): **ssh tim@host**

*Different nodes available for different use cases*

<!--v-->

![overview of different login nodes](login-nodes.png) <!-- .element height="80%" width="80%" -->

<!--v-->

<!-- .slide: data-auto-animate -->
### RWTH Compute Cluster <!-- .element: data-id="code-animation"-->
#### File Systems

![](filesystems.png)

- `$HOME` for code/configuration, `$WORK` for output and working data, `$HPCWORK` for large data

<!--v-->

<!-- .slide: data-auto-animate -->
### RWTH Compute Cluster <!-- .element: data-id="code-animation"-->
#### Project Management

- Groups are created for every project
- Every group consists of owners (PC/PI), Managers and Members
- Granted computation time and storage space is shared through groups
- In Aachen project storage is deleted **8 months** after a project's conclusion. Make sure you migrate the data by then!

<!--v-->

<!-- .slide: data-auto-animate -->
### RWTH Compute Cluster <!-- .element: data-id="code-animation"-->
#### Project Management

Users can be added to and removed from groups/projects using their TIM-ID.
```sh
member add --name <project-id> <user-id>
member delete --name <project-id> <user-id>
member finger # view group affiliations
```
- append **--manager** to any command above to assign or revoke the manager role
---

<!-- .slide: data-auto-animate -->
## Tips and Tricks <!-- .element: data-id="code-animation"-->

This is a collection of tips and tricks that make working with the cluster easier and more convenient.

<!--v-->

<!-- .slide: data-auto-animate -->
## Tips and Tricks <!-- .element: data-id="code-animation"-->

1. You can mount the remote cluster file system to your local machine

```zsh
sshfs ab1234@copy23node:/home/ab1234 /mnt/clusterhome
```
<!-- .element: class="fragment" data-fragment-index="1" -->
<!--v-->

<!-- .slide: data-auto-animate -->
## Tips and Tricks <!-- .element: data-id="code-animation"-->

1. You can mount the remote cluster file system to your local machine

```zsh
sshfs ab1234@copy23node:/home/ab1234 /mnt/clusterhome
```

unmount with: 
```zsh
sudo umount -l /mnt/clusterhome
```

<!--v-->

<!-- .slide: data-auto-animate -->

## Tips and Tricks <!-- .element: data-id="code-animation"-->

2. If you are outside of eduroam, you can connect through VPN and access the cluster *everywhere*
    
Go to http://help.itc.rwth-aachen.de  
and search for "VPN"

---

<!-- .slide: data-auto-animate -->

## SLURM Job Manager <!-- .element: data-id="code-animation"-->

**S**imple **L**inux **U**tility for **R**esource **M**anagement

- Job scheduler often used in supercomputers and compute clusters <!-- .element: class="fragment" data-fragment-index="1" -->
- Provides many advantages for utilizing HPC hardware with many users, such as..  <!-- .element: class="fragment" data-fragment-index="2" -->
  - ... Accounting, Containerization, Priorities, Chain- and Array-Jobs, ... <!-- .element: class="fragment" data-fragment-index="2" -->

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

- Users can interact with SLURM from login nodes <!-- .element: class="fragment" data-fragment-index="1" -->
- Users may request cores, memory and time, then send their programs to be queued  <!-- .element: class="fragment" data-fragment-index="2" -->
- SLURM reserves these resources and waits till they are available <!-- .element: class="fragment" data-fragment-index="3" -->
- Once available, the code will then be run <!-- .element: class="fragment" data-fragment-index="4" -->

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

![](slurm.png)

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

SLURM is fed **jobscripts**, which contain all information the scheduler needs to run a program

These jobscripts consist of three parts:

1. Shebang <!-- .element: class="fragment" data-fragment-index="1" -->
2. Job Parameters <!-- .element: class="fragment" data-fragment-index="2" --> 
3. Actual Job Code <!-- .element: class="fragment" data-fragment-index="3" -->

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Example

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Example

```sh
#!/usr/bin/zsh
```
<!-- .element: data-id="code-animation"-->

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Example

```sh
#!/usr/bin/zsh

### Job Parameters
#SBATCH --cpus-per-task=8              
#SBATCH --time=00:15:00         
#SBATCH --job-name=example_job  
#SBATCH --output=stdout.txt     
#SBATCH --account=<project-id>
```
<!-- .element: data-id="code-animation"-->

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Example

```sh
#!/usr/bin/zsh

### Job Parameters
#SBATCH --cpus-per-task=8              
#SBATCH --time=00:15:00         
#SBATCH --job-name=example_job  
#SBATCH --output=stdout.txt     
#SBATCH --account=<project-id>

### Program Code
echo "Hello SLURM"
```
<!-- .element: data-id="code-animation"-->

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Example

- Safe the file
- Submit the job <!-- .element: class="fragment" data-fragment-index="1" -->
```sh
> sbatch testjob.sh
```
<!-- .element: class="fragment" data-fragment-index="2" -->
- Check its state <!-- .element: class="fragment" data-fragment-index="3" -->
```sh
> squeue --me
JOBID     PARTITION  NAME        USER      ST    TIME  NODES 
12345678  c23ms      example_job AB123456  R     0:02      1
```
<!-- .element: class="fragment" data-fragment-index="4" -->

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Common Parameters

- Number of cores: **-c /--cpus-per-task \<numpcus>**
- Memory: **-m /--mem=\<amount>G**
- Human readable Job name: **-J /--job-name**
- Reporting File: **-o /--output=\<filename>**
- Runtime: **-t /--time=d-hh:mm:ss**
- Account: **-A /--account=\<project>**
- GPUs: **--gres=gpu:\<type>:\<amount>**


<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Commands in a Nutshell

Submit jobs
```sh
> sbatch <BATCH_SCRIPT> [ADDITIONAL ARGUMENTS]
```

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Commands in a Nutshell

Display Job Queue
```sh
> squeue [OPTIONS]
```
**--me** shows only your jobs  
**--start** shows the estimated starttime  
**--format** can be used to filter e.g. for GPU jobs

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Commands in a Nutshell

Cancel Jobs
```sh
> scancel [OPTIONS] <JOB-ID>
```
**--me** cancel all of your jobs  
**--v** provide details of the cancellation process  

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Commands in a Nutshell

Request an interactive job
```sh
> salloc [OPTIONS]
```
Job parameters just like in the jobscript. Redirects the shell to the head node. Job ends when the shell terminates.
```sh
salloc --gres=gpu:1 -n 24 -t 1:00:00 # 24c+GPU node for 1 hour
```
<!-- .element: class="fragment" data-fragment-index="1" -->

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Commands in a Nutshell

Display Accounting Information
```sh
> sacct [OPTIONS]
```
Print details abound pending, running, or past jobs.
```sh
> sacct -S $(date -I --date="yesterday")
```
Would show all jobs submitted since yesterday.

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

#### Commands in a Nutshell
```sh
> r_wlm_usage
```
Command for only the RWTH HPC to display available / used resources against the account / group quota

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

Monitoring tools are improving constantly, with *perfmon* currently being established across compute clusters

<!--v-->

<!-- .slide: data-auto-animate -->
## SLURM Job Manager <!-- .element: data-id="code-animation"-->

![](grafana.png)

---

<!-- .slide: data-auto-animate -->
## Compute Time Application <!-- .element: data-id="code-animation"-->
- Resources are measured in core hours (core-h) <!-- .element: class="fragment" data-fragment-index="1" -->
    - A Macbook Pro has roughly 70k core-h/year <!-- .element: class="fragment" data-fragment-index="2" -->
- The smallest project already guarantees 360k core-h <!-- .element: class="fragment" data-fragment-index="3" -->
- Estimate your needs based on previous work <!-- .element: class="fragment" data-fragment-index="4" -->
- Write and submit your application! <!-- .element: class="fragment" data-fragment-index="5" -->

<!--v-->

<!-- .slide: data-auto-animate -->
## Compute Time Application <!-- .element: data-id="code-animation"-->

HPC resources in Germany are arranged hierarchically in the HPC Performance Pyramid.
- **Tier-0**: PRACE and EuroHPC
- **Tier-1**: Gauss Center for Supercomputing (GCS, JSC, HLRS, LRZ)
- **Tier-2**: HPC Centres with supra-regional tasks and thematically dedicated HPC/centres
- **Tier-3**: Regional HPC centres

<!--v-->

<!-- .slide: data-auto-animate -->
## Compute Time Application <!-- .element: data-id="code-animation"-->

![alt text](image-2.png)

<!--v-->

<!-- .slide: data-auto-animate -->
## Compute Time Application <!-- .element: data-id="code-animation"-->

![](computetime.jpg)

<!--v-->

![](nhr.png)

<!--v-->

![](nhrprocess.jpg)

---

### Acknowledgement (RWTH/JARA)

*Computations were performed with computing resources granted by RWTH Aachen University under project <ID of your project>.*

---

### Acknowledgement (RWTH/JARA)

*The authors gratefully acknowledge the computing time provided to them at the NHR Center NHR4CES at RWTH Aachen University (project number <your-project-id: p0020XXX>). This is funded by the Federal Ministry of Education and Research, and the state governments participating on the basis of the resolutions of the GWK for national high performance computing at universities (www.nhr-verein.de/unsere-partner).*
          </textarea>
        </section>
      </div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./mermaid/dist/mermaid.min.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        slideNumber: true,
        highlight: {
          highlightOnLoad: false
        },
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"transition":"slide","auto-animate":true,"_":["slides.md"],"static":"_site"}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
      Reveal.addEventListener('ready', function (event) {
        const blocks = Reveal.getRevealElement().querySelectorAll('pre code:not(.mermaid)');
        const hlp = Reveal.getPlugin('highlight');
        blocks.forEach(hlp.highlightBlock);
      });
    </script>

    <script>
      const mermaidOptions = extend({ startOnLoad: false }, {});
      mermaid.startOnLoad = false;
      mermaid.initialize(mermaidOptions);
      const cb = function (event) { mermaid.init(mermaidOptions, '.stack.present>.present pre code.mermaid'); };
      Reveal.addEventListener('ready', cb);
      Reveal.addEventListener('slidetransitionend', cb);
    </script>
  </body>
</html>
